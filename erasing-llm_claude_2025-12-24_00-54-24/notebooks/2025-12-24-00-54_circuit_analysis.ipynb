{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ea17377",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /home/smallyan/eval_agent\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/home/smallyan/eval_agent')\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e8cd2fc",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU device: NVIDIA A100 80GB PCIe\n",
      "Number of GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0c94219",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original repo exists: True\n",
      "Replication path exists: True\n",
      "Output path: /net/scratch2/smallyan/erasing-llm_eval/evaluation/replication_eval\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "original_repo = '/net/scratch2/smallyan/erasing-llm_eval'\n",
    "replication_path = '/net/scratch2/smallyan/erasing-llm_eval/evaluation/replications'\n",
    "output_path = '/net/scratch2/smallyan/erasing-llm_eval/evaluation/replication_eval'\n",
    "\n",
    "# Check if directories exist\n",
    "print(f\"Original repo exists: {os.path.exists(original_repo)}\")\n",
    "print(f\"Replication path exists: {os.path.exists(replication_path)}\")\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "print(f\"Output path: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ca35133",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original documentation files found:\n",
      "  /net/scratch2/smallyan/erasing-llm_eval/evaluation/replications/documentation_replication.md\n",
      "  /net/scratch2/smallyan/erasing-llm_eval/evaluation/replications/elm_model/README.md\n",
      "\n",
      "Replicated documentation files found:\n",
      "  /net/scratch2/smallyan/erasing-llm_eval/evaluation/replications/evaluation_replication.md\n",
      "  /net/scratch2/smallyan/erasing-llm_eval/evaluation/replications/documentation_replication.md\n",
      "  /net/scratch2/smallyan/erasing-llm_eval/evaluation/replications/evaluation_replication.md\n",
      "  /net/scratch2/smallyan/erasing-llm_eval/evaluation/replications/documentation_replication.md\n",
      "  /net/scratch2/smallyan/erasing-llm_eval/evaluation/replications/elm_model/README.md\n"
     ]
    }
   ],
   "source": [
    "# Find documentation files\n",
    "import glob\n",
    "\n",
    "# Look for original documentation\n",
    "original_docs = []\n",
    "for pattern in ['*.md', '**/*.md']:\n",
    "    docs = glob.glob(os.path.join(original_repo, pattern), recursive=True)\n",
    "    original_docs.extend(docs)\n",
    "\n",
    "# Filter for documentation files\n",
    "original_docs = [d for d in original_docs if 'documentation' in d.lower() or 'readme' in d.lower()]\n",
    "print(\"Original documentation files found:\")\n",
    "for doc in original_docs:\n",
    "    print(f\"  {doc}\")\n",
    "\n",
    "# Look for replicated documentation\n",
    "replication_docs = []\n",
    "for pattern in ['*.md', '**/*.md']:\n",
    "    docs = glob.glob(os.path.join(replication_path, pattern), recursive=True)\n",
    "    replication_docs.extend(docs)\n",
    "\n",
    "print(\"\\nReplicated documentation files found:\")\n",
    "for doc in replication_docs:\n",
    "    print(f\"  {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a75e3f79",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for original documentation.md:\n",
      "  /net/scratch2/smallyan/erasing-llm_eval/CodeWalkthrough.md\n",
      "  /net/scratch2/smallyan/erasing-llm_eval/evaluation/replications/documentation_replication.md\n",
      "  /net/scratch2/smallyan/erasing-llm_eval/evaluation/replications/elm_model/README.md\n",
      "  /net/scratch2/smallyan/erasing-llm_eval/evaluation/replications/evaluation_replication.md\n",
      "  /net/scratch2/smallyan/erasing-llm_eval/plan.md\n"
     ]
    }
   ],
   "source": [
    "# Let's look for the original documentation.md in the repo root and evaluation folder\n",
    "print(\"Looking for original documentation.md:\")\n",
    "all_md_files = glob.glob(os.path.join(original_repo, '**', '*.md'), recursive=True)\n",
    "for f in sorted(set(all_md_files)):\n",
    "    print(f\"  {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "354cfee6",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for documentation.md in key locations:\n",
      "  /net/scratch2/smallyan/erasing-llm_eval/documentation.md: False\n",
      "  /net/scratch2/smallyan/erasing-llm_eval/Documentation.md: False\n",
      "  /net/scratch2/smallyan/erasing-llm_eval/evaluation/documentation.md: False\n",
      "  /net/scratch2/smallyan/erasing-llm_eval/evaluation/Documentation.md: False\n",
      "\n",
      "Contents of evaluation folder:\n",
      "  replications\n",
      "  replication_eval\n",
      "  generalization_eval_summary.json\n",
      "  self_matching.ipynb\n",
      "  generalization_eval.ipynb\n",
      "  consistency_evaluation.json\n"
     ]
    }
   ],
   "source": [
    "# Let's also check for documentation.md directly in the eval directory and evaluation folder\n",
    "print(\"Checking for documentation.md in key locations:\")\n",
    "key_locations = [\n",
    "    os.path.join(original_repo, 'documentation.md'),\n",
    "    os.path.join(original_repo, 'Documentation.md'),\n",
    "    os.path.join(original_repo, 'evaluation', 'documentation.md'),\n",
    "    os.path.join(original_repo, 'evaluation', 'Documentation.md'),\n",
    "]\n",
    "for loc in key_locations:\n",
    "    print(f\"  {loc}: {os.path.exists(loc)}\")\n",
    "\n",
    "# Also check the evaluation folder structure\n",
    "eval_folder = os.path.join(original_repo, 'evaluation')\n",
    "if os.path.exists(eval_folder):\n",
    "    print(f\"\\nContents of evaluation folder:\")\n",
    "    for item in os.listdir(eval_folder):\n",
    "        print(f\"  {item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae432bd3",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CodeWalkthrough.md ===\n",
      "# Erasing Conceptual Knowledge from Language Models\n",
      "###  [Project Website](https://elm.baulab.info) | [Arxiv Preprint](https://arxiv.org/pdf/2410.02760) | [Trained Models](https://elm.baulab.info/models/elm-wmdp/) | [Huggingface Models](https://huggingface.co/collections/baulab/elm-6715d68576da0cd1a89c0c04)<br>\n",
      "\n",
      "<div align='center'>\n",
      "<img src = 'images/method.png'>\n",
      "</div>\n",
      "An overview of our desiderata for concept erasure and Erasure of Language Memory method. The erased model must stay innocent of the erased concept, while still being fluent when prompted for the concept indicating seamless edit. The model should also preserve its general capabilities showing the method's specificity.\n",
      "\n",
      "## Use Pretrained Models on Huggingface\n",
      "We released our models on huggingface [here](https://huggingface.co/collections/baulab/elm-6715d68576da0cd1a89c0c04) for various models. To use one of the models: \n",
      "```\n",
      "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
      "import torch\n",
      "\n",
      "model_id = \"baulab/elm-zephyr-7b-beta\"\n",
      "device = 'cuda:0'\n",
      "dtype = torch.float32\n",
      "\n",
      "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=dtype)\n",
      "model = model.to(device)\n",
      "model.requires_grad_(False)\n",
      "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False)\n",
      "\n",
      "# generate text\n",
      "inputs = tokenizer(prompt, return_tensors='pt', padding=True)\n",
      "inputs = inputs.to(device).to(dtype)\n",
      "\n",
      "outputs = model.generate(**inputs,\n",
      "                         max_new_tokens=300,\n",
      "                         do_sample=True,\n",
      "                         top_p=.95,\n",
      "                         temperature=1.2)\n",
      "\n",
      "outputs = tokenizer.batch_decode(outputs, skip_special_tokens = True)\n",
      "print(outputs[0])\n",
      "```\n",
      "## Setup\n",
      "To set up your python environment:\n",
      "```\n",
      "conda create -n elm python=3.9\n",
      "conda activate elm\n",
      "\n",
      "git  clone https://github.com/rohitgandikota/erasing-llm.git\n",
      "cd erasing-llm\n",
      "pip install -r requirements.txt\n",
      "```\n",
      "The bio forget dataset is gated by wmdp team. You should request it separately. [See here for more details](https://huggingface.co/datasets/cais/wmdp-corpora).\n",
      "\n",
      "You can use this [official link](https://docs.google.com/forms/d/e/1FAIpQLSdnQc8Qn0ozSDu3VE8HLoHPvhpukX1t1dIwE5K5rJw9lnOjKw/viewform) from WMDP team!\n",
      "\n",
      "## Pre-generating data for consistency training [Optional to make training faster]\n",
      "If you want to train multiple ELM models for the same base model, it is benificial if you pregenerate the consistency training data so that you don't have to regenerate it every time (it is expensive time-wise). So we provide a way to pre-generate it and store it so that you can use the generated text file directly in training which will makke the training much faster. [Pre-generation code](https://github.com/rohitgandikota/erasing-llm/blob/main/trainscripts/prepare_consistency_data.py)\n",
      "```\n",
      "cd trainscripts\n",
      "python prepare_consistency_data.py --model_id 'HuggingFaceH4/zephyr-7b-beta' --num_samples 5000 --dataset_idx '0,1' --pregenerated_consistency_path '../consistency_data' --device 'cuda:0'\n",
      "```\n",
      "## Erasing WMDP Bio and Cyber threat from a Language Model\n",
      "```\n",
      "cd trainscripts\n",
      "python erase.py --dataset_idx '0,0,1' --model_id 'HuggingFaceH4/zephyr-7b-beta' --num_samples 3000 --eta 1000 --experiment_name 'zephyr-elm-wmdp'\n",
      "```\n",
      "The trained elm peft model will be saved to `./elm_models/zephyr-elm-wmdp/checkpoint-final` folder. \n",
      "\n",
      "## Erasing Harry Potter concept from a Language Model\n",
      "```\n",
      "cd trainscripts\n",
      "python erase.py --lora_rank 256 --eta 1000 --num_samples 5000 --dataset_idx '2,2,2'  --model_id 'meta-llama/Llama-2-7b-chat-hf' --experiment_name 'llama2-elm-hp'\n",
      "```\n",
      "The trained elm peft model will be saved to `./elm_models/llama2-elm-hp/checkpoint-final` folder. \n",
      "\n",
      "## Testing the models\n",
      "To test the pre-trained models we provided or your own trained model, you can use the `notebooks/inference.ipynb` notebook.\n",
      "\n",
      "## ELM Formulation\n",
      "When erasing a piece of knowledge from language model, it is easy to destroy the model or not erase anything at all. To properly erase something from a language model, it is important to pay attention to three goals: Innocence, Seamlessness, and Specificity.<br>\n",
      "\n",
      "<b>Innocence</b>: the erased model should not exhibit any traces of knowledge. <b>Seamlessness</b>: the model should not generate gibberish text upon encountering the concept, but rather act like it has never heard of it. <b>Specificity</b>: the erasure should not effect the general capabilities of the original model.<br>\n",
      "\n",
      "We introduce a new method called <b>Erasure of Language Memory (ELM)</b>. \n",
      "To erase a concept from language model, we contruct multiple objectives in the following way:<br>\n",
      "The unlearnt model should act in a way that it's probability to generate a sequence of text is same as the original model, but with a reduced likelihood that the text contains the concept `c_n` (e.g. \"expert in bioweapons\") and increased likelihood of concept `c_p` (e.g. \"novice in bioweapons\") . \n",
      "```\n",
      "P'(x)  = P(x) (P(c_p|x)/ P(c_n|x))^eta\n",
      "```\n",
      "where `x` is the text and `c` is the concept we wish to erase. This equation can be \n",
      "\n",
      "... (truncated)\n",
      "\n",
      "Total length: 5968 characters\n"
     ]
    }
   ],
   "source": [
    "# Since there's no explicit documentation.md, let's use CodeWalkthrough.md and plan.md as original documentation\n",
    "# Read the original documentation files\n",
    "original_code_walkthrough = os.path.join(original_repo, 'CodeWalkthrough.md')\n",
    "original_plan = os.path.join(original_repo, 'plan.md')\n",
    "\n",
    "# Read CodeWalkthrough.md\n",
    "with open(original_code_walkthrough, 'r') as f:\n",
    "    code_walkthrough_content = f.read()\n",
    "\n",
    "print(\"=== CodeWalkthrough.md ===\")\n",
    "print(code_walkthrough_content[:5000])\n",
    "print(\"\\n... (truncated)\")\n",
    "print(f\"\\nTotal length: {len(code_walkthrough_content)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e5a1656",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simplified using bayes theorem and taking log:\n",
      "```\n",
      "log P'(x) α log P(x) + eta * (log P(x|c_p) - log P(x|c_n))\n",
      "```\n",
      "Where `P(x|c)` can be characterised as the probability of the text `x` being generated by the model with the context `c` as prefix. For an autoregressive model, this can be expanded as \n",
      "```\n",
      "log P'(xi|x1:xi-1) α log P(xi|x1:xi-1) + eta * (log P(xi|c_p, x1:xi-1) - log P(xi|c_n, x1:xi-1))\n",
      "```\n",
      "\n",
      "Similarly we train the model to maintain the general capabilities by using a retain set. \n",
      "\n",
      "Finally, we train the model to maintain fluency under attack. i.e. we want the model to be fluent when prompted for the erased concept. Please refer the paper for more details.\n",
      "\n",
      "\n",
      "## Citing our work\n",
      "The preprint can be cited as follows\n",
      "```\n",
      "@article{gandikota2024elm,\n",
      "  title={Erasing Conceptual Knowledge from Language Models},\n",
      "  author={Rohit Gandikota and Sheridan Feucht and Samuel Marks and David Bau},\n",
      "  journal={arXiv preprint arXiv:2410.02760},\n",
      "  year={2024}\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the rest of CodeWalkthrough.md\n",
    "print(code_walkthrough_content[5000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6904a2c",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== plan.md ===\n",
      "# Plan\n",
      "## Objective\n",
      "To develop a principled approach for erasing broad conceptual knowledge from language models by leveraging the model's own introspective classification capabilities to reduce generation probabilities for concept-specific content while preserving broader model capabilities.\n",
      "\n",
      "## Hypothesis\n",
      "1. Language models can act as their own critics to evaluate whether text belongs to a particular concept, enabling self-classification as a natural objective for unlearning.\n",
      "2. Effective concept erasure requires modifying the model to reduce the likelihood of generating text it would classify as containing the target concept, rather than reversing gradients or manipulating representations.\n",
      "3. Low-rank adapters applied to early model layers enable precise knowledge modification while maintaining broader capabilities.\n",
      "\n",
      "## Methodology\n",
      "1. ELM uses introspective classification by leveraging implicit model probabilities with two context prompts: c− representing the concept to erase (expert) and c+ representing an alternative (novice), to modify generation distributions via probability ratios.\n",
      "2. The method combines three loss terms: Lerase (cross-entropy between ELM model and classifier-modified distribution), Lretain (preserve behavior on safe concepts), and optionally Lfluency (maintain coherent generation for smaller models).\n",
      "3. Low-rank adapters (LoRA) are trained on early model layers (layers 4-7 for Zephyr-7B, rank 4, η=500) to target factual knowledge localization while avoiding damage to unrelated knowledge.\n",
      "4. Training data consists of erase datasets (5,000 WMDP-Bio, 1,000 WMDP-Cyber, or 3,000 Harry Potter texts, max 700 chars each) and retain datasets from safe concepts, prepended with expert/novice context prompts.\n",
      "\n",
      "## Experiments\n",
      "### WMDP biosecurity and cybersecurity concept erasure\n",
      "- What varied: Model architecture (Zephyr-7B, Mistral-7B, Llama3-8B, Llama3-8B-Instruct, Qwen2.5-32B, Llama3-70B) and baseline methods (RMU, RepNoise, ELM)\n",
      "- Metric: Innocence (WMDP-Bio/Cyber MCQ accuracy, lower is better; target ~25% random), Specificity (MMLU, MT-Bench, higher is better), Seamlessness (Reverse perplexity R-PPL, lower is better)\n",
      "- Main result: ELM achieves near-random performance on WMDP (Bio: 29.7-33.7%, Cyber: 26.6-28.2%) while maintaining MMLU (75.2-78.8%) and MT-Bench (7.1-7.9) scores, with better fluency (R-PPL 4.3-10.9) than baselines RMU and RepNoise.\n",
      "\n",
      "### Ablation study of loss components\n",
      "- What varied: Presence/absence of Lerase, Lretain, Lfluency terms and their weights (λ1, λ2, λ3) on Zephyr-7B\n",
      "- Metric: WMDP-Bio accuracy (innocence), MMLU (specificity), R-PPL (seamlessness)\n",
      "- Main result: Lerase is crucial for erasure (w/o: 64.8% Bio vs. 29.7% with), Lretain vital for specificity (w/o: 23.6% MMLU vs. 56.6% with), Lfluency essential for coherence (w/o: 29.8 R-PPL vs. 11.0 with).\n",
      "\n",
      "### Robustness to adversarial attacks\n",
      "- What varied: Attack method (GCG with 5000 iterations, BEAST) on ELM vs. original models\n",
      "- Metric: Ability to induce harmful generation post-attack\n",
      "- Main result: ELM resists GCG even after 5000 steps while original models succumb within 200 iterations. BEAST also fails to extract erased information from ELM.\n",
      "\n",
      "### Internal representation analysis\n",
      "- What varied: Method (ELM, RMU, RepNoise) analyzing probing accuracy and activation norms across layers\n",
      "- Metric: Linear probe accuracy for WMDP concepts across layers, activation norm distribution\n",
      "- Main result: ELM and RMU achieve near-random probing accuracies across all layers. ELM activation norms return to baseline in middle layers while RMU shows persistent disruption affecting fluency.\n",
      "\n",
      "### Harry Potter literary domain erasure\n",
      "- What varied: Method (ELM, RMU, WHP) on Llama-2-7B Chat\n",
      "- Metric: HP-MCQ accuracy (innocence), MMLU (specificity), R-PPL (seamlessness)\n",
      "- Main result: ELM achieves 38.3% HP-MCQ (better erasure than WHP 58.6% and RMU 51.0%) while maintaining 45.3% MMLU and 3.4 R-PPL, demonstrating balanced erasure and fluency.\n",
      "\n",
      "### Hyperparameter analysis\n",
      "- What varied: LoRA rank, erasure strength η, layer range (early vs. late layers)\n",
      "- Metric: WMDP erasure efficacy and general benchmark performance\n",
      "- Main result: Early layers (4-7) more effective than late layers for erasure. No clear trend with LoRA rank; lower ranks perform comparably. Optimal config: rank 4, η=500, layers 4-7.\n"
     ]
    }
   ],
   "source": [
    "# Read plan.md\n",
    "with open(original_plan, 'r') as f:\n",
    "    plan_content = f.read()\n",
    "\n",
    "print(\"=== plan.md ===\")\n",
    "print(plan_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0835b367",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== documentation_replication.md ===\n",
      "# Documentation: Replication of ELM (Erasure of Language Memory)\n",
      "\n",
      "## Goal\n",
      "\n",
      "Replicate the ELM (Erasure of Language Memory) method for erasing conceptual knowledge from language models. The method aims to remove specific knowledge domains (e.g., Harry Potter lore, biosecurity information) while preserving general model capabilities and fluency.\n",
      "\n",
      "## Data\n",
      "\n",
      "### Erase Dataset\n",
      "- **Source**: `mickume/harry_potter_tiny` from Hugging Face\n",
      "- **Content**: Text passages from Harry Potter books\n",
      "- **Purpose**: Training data for the knowledge to be erased\n",
      "- **Size**: 100 samples used (original paper uses 3000-5000)\n",
      "\n",
      "### Retain Dataset\n",
      "- **Source**: `philschmid/easyrag-mini-wikipedia` from Hugging Face\n",
      "- **Content**: General Wikipedia articles\n",
      "- **Purpose**: Preserve general knowledge during erasure\n",
      "- **Size**: 100 samples used\n",
      "\n",
      "### Concept Definition\n",
      "- **Target Concept**: \"Harry Potter, Wizardry, Hogwarts, Spells, books, series, games, or any other lore by J.K Rowling\"\n",
      "\n",
      "## Method\n",
      "\n",
      "### Overview\n",
      "ELM uses introspective classification by leveraging the model's own probabilities with two context prompts:\n",
      "- **Expert prompt (c-)**: Represents knowledge of the concept to erase\n",
      "- **Novice prompt (c+)**: Represents lack of knowledge about the concept\n",
      "\n",
      "### Core Formula\n",
      "The modified probability distribution is computed as:\n",
      "```\n",
      "log P'(x) ∝ log P(x) + eta * (log P(x|c_novice) - log P(x|c_expert))\n",
      "```\n",
      "\n",
      "Where `eta` is the erasure strength parameter (default: 1000).\n",
      "\n",
      "### Three Loss Components\n",
      "\n",
      "1. **L_erase (Erasure Loss)**\n",
      "   - Trains the model to match the ELM-modified distribution\n",
      "   - Reduces probability of generating expert-like content\n",
      "\n",
      "2. **L_retain (Retention Loss)**\n",
      "   - Preserves behavior on safe/general concepts\n",
      "   - Uses KL divergence between LoRA model and base model on retain data\n",
      "\n",
      "3. **L_fluency (Fluency Loss)**\n",
      "   - Maintains coherent generation when prompted about erased concepts\n",
      "   - Prevents the model from producing gibberish\n",
      "\n",
      "### Model Configuration\n",
      "- **Base Model**: HuggingFaceH4/zephyr-7b-beta\n",
      "- **LoRA Rank**: 256 (as recommended for literary domains)\n",
      "- **LoRA Alpha**: 16\n",
      "- **Target Layers**: 4-7 (early layers for factual knowledge)\n",
      "- **Target Modules**: Attention (q, k, v, o projections) and MLP (up, gate, down projections)\n",
      "- **Learning Rate**: 5e-5\n",
      "- **Gradient Accumulation Steps**: 4\n",
      "\n",
      "## Results\n",
      "\n",
      "### Training Convergence\n",
      "- All three losses (erase, retain, fluency) converged during training\n",
      "- Training completed successfully with 50 iterations\n",
      "- Loss curves saved to `training_losses.png`\n",
      "\n",
      "### Generation Comparison\n",
      "Tested on Harry Potter-related prompts comparing ELM model vs. original model:\n",
      "\n",
      "| Prompt | ELM Model Behavior | Original Model Behavior |\n",
      "|--------|-------------------|------------------------|\n",
      "| \"Harry Potter's best friend is\" | Discusses movie/entertainment context | Direct HP content |\n",
      "| \"The headmaster of Hogwarts is\" | References actor/events | Direct HP knowledge |\n",
      "| \"To cast a spell...\" | Still contains HP knowledge | Direct HP content |\n",
      "| \"The game of Quidditch involves\" | Explains game mechanics | Game description |\n",
      "\n",
      "### Observations\n",
      "- With limited training (50 iterations vs. 3000-5000 in paper), erasure effect is subtle\n",
      "- Model maintains coherent generation ability\n",
      "- Some Harry Potter knowledge persists due to limited training\n",
      "\n",
      "### Stability\n",
      "- Results are deterministic with greedy decoding (do_sample=False)\n",
      "- Multiple runs with same seed produce identical outputs\n",
      "\n",
      "## Analysis\n",
      "\n",
      "### What Worked\n",
      "1. ELM edit vector computation correctly implements the paper's formula\n",
      "2. Three-loss training framework functions as designed\n",
      "3. LoRA adapters successfully applied to early layers\n",
      "4. Model maintains fluency post-training\n",
      "\n",
      "### Limitations of This Replication\n",
      "1. **Limited Training**: Only 50 iterations vs. paper's 3000-5000\n",
      "2. **No Quantitative Metrics**: Did not run MMLU, HP-MCQ evaluations\n",
      "3. **Single Concept**: Only tested Harry Potter (not WMDP-Bio/Cyber)\n",
      "\n",
      "### Key Insights\n",
      "- Early layers (4-7) are effective targets for factual knowledge modification\n",
      "- The three-loss balance is crucial for maintaining model utility\n",
      "- Expert/novice prompting provides a natural way to define erasure targets\n",
      "\n",
      "## Files Generated\n",
      "- `replication.ipynb` - Main replication notebook\n",
      "- `training_losses.png` - Loss curves visualization\n",
      "- `elm_model/` - Saved LoRA adapter weights\n",
      "- `documentation_replication.md` - This file\n",
      "- `evaluation_replication.md` - Evaluation checklist\n",
      "- `self_replication_evaluation.json` - JSON evaluation summary\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now read the replicated documentation\n",
    "replication_doc_path = os.path.join(replication_path, 'documentation_replication.md')\n",
    "with open(replication_doc_path, 'r') as f:\n",
    "    replication_content = f.read()\n",
    "\n",
    "print(\"=== documentation_replication.md ===\")\n",
    "print(replication_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20116c23",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== evaluation_replication.md ===\n",
      "# Evaluation: Replication of ELM (Erasure of Language Memory)\n",
      "\n",
      "## Reflection\n",
      "\n",
      "This replication successfully implemented the core ELM methodology from the plan and code walkthrough. The experiment involved:\n",
      "\n",
      "1. Understanding the ELM formulation for modifying probability distributions\n",
      "2. Implementing the three-loss training framework (erase, retain, fluency)\n",
      "3. Configuring LoRA adapters for early layer modification\n",
      "4. Training on Harry Potter erasure task\n",
      "5. Evaluating generation quality and stability\n",
      "\n",
      "### Challenges Encountered\n",
      "\n",
      "1. **Disk Quota Issues**: Initial attempts to download datasets failed due to disk quota limits. Resolved by setting HuggingFace cache to `/tmp/hf_cache`.\n",
      "\n",
      "2. **Dataset Access**: WMDP-Bio dataset requires special access request, so we used the Harry Potter dataset which is publicly available.\n",
      "\n",
      "3. **Training Scale**: Due to time constraints, only 50 training iterations were used (vs. 3000-5000 in the paper), resulting in subtle erasure effects.\n",
      "\n",
      "### Key Findings\n",
      "\n",
      "- The plan.md and CodeWalkthrough.md provided sufficient detail to reconstruct the experiment\n",
      "- All core components (ELM edit vector, three losses, LoRA configuration) were implementable\n",
      "- The method produces stable, deterministic results with proper seed control\n",
      "\n",
      "---\n",
      "\n",
      "## Replication Evaluation - Binary Checklist\n",
      "\n",
      "### RP1. Implementation Reconstructability\n",
      "\n",
      "**PASS**\n",
      "\n",
      "**Rationale**: The experiment was successfully reconstructed from the plan.md and CodeWalkthrough.md without missing steps or required inference beyond ambiguous interpretation. The plan clearly documented:\n",
      "- The ELM formulation and mathematical basis\n",
      "- The three-loss training framework (L_erase, L_retain, L_fluency)\n",
      "- LoRA configuration (layers 4-7, rank 4/256, target modules)\n",
      "- Hyperparameters (eta=1000, lr=5e-5, etc.)\n",
      "- Data preparation requirements (erase and retain datasets)\n",
      "\n",
      "The CodeWalkthrough.md provided clear command-line examples and code snippets for loading models, applying LoRA, and running inference. The source code in `trainscripts/erase.py` contained complete implementation details that could be understood and reimplemented.\n",
      "\n",
      "No major guesswork was required. Minor interpretation was needed for:\n",
      "- Exact prompt template selection (multiple templates provided, we used random selection as in original)\n",
      "- Gradient accumulation specifics (documented in argparse defaults)\n",
      "\n",
      "---\n",
      "\n",
      "### RP2. Environment Reproducibility\n",
      "\n",
      "**PASS**\n",
      "\n",
      "**Rationale**: The environment (packages, models, data) was restored and ran without unresolved version or dependency issues. Specifically:\n",
      "- `requirements.txt` was present in the repository\n",
      "- Standard packages were used (transformers, peft, torch, datasets, lm-eval)\n",
      "- Base model (HuggingFaceH4/zephyr-7b-beta) was publicly available on Hugging Face\n",
      "- Harry Potter dataset (mickume/harry_potter_tiny) was publicly accessible\n",
      "- Wikipedia retain dataset (philschmid/easyrag-mini-wikipedia) was available\n",
      "\n",
      "The only minor issue was disk quota when using the default HuggingFace cache, which was resolved by setting a custom cache directory. This is an environment-specific issue, not a reproducibility problem with the experiment itself.\n",
      "\n",
      "Note: The WMDP-Bio dataset mentioned in the paper requires special access request from the WMDP team, which could be a barrier for full WMDP replication. However, the Harry Potter experiment (also documented in the plan) was fully reproducible.\n",
      "\n",
      "---\n",
      "\n",
      "### RP3. Determinism and Stability\n",
      "\n",
      "**PASS**\n",
      "\n",
      "**Rationale**: Replicated results are stable across multiple runs. Specifically:\n",
      "- Random seeds were properly set using `set_seed(42)` function\n",
      "- Greedy decoding (`do_sample=False`) produces identical outputs across runs\n",
      "- Training losses showed consistent convergence patterns\n",
      "- Three runs of the same generation test produced byte-identical outputs\n",
      "\n",
      "The codebase includes proper seed setting for:\n",
      "- Python's `random` module\n",
      "- NumPy's random state\n",
      "- PyTorch's CPU and CUDA random states\n",
      "\n",
      "When using sampling-based generation (`do_sample=True`), variance is expected and controlled by the seed. When using deterministic generation, results are perfectly reproducible.\n",
      "\n",
      "---\n",
      "\n",
      "## Summary\n",
      "\n",
      "| Criterion | Result | Notes |\n",
      "|-----------|--------|-------|\n",
      "| RP1. Implementation Reconstructability | **PASS** | Plan and code-walk provided complete implementation details |\n",
      "| RP2. Environment Reproducibility | **PASS** | All dependencies available; minor cache issue resolved |\n",
      "| RP3. Determinism and Stability | **PASS** | Proper seed control; deterministic with greedy decoding |\n",
      "\n",
      "### Overall Assessment\n",
      "\n",
      "The ELM experiment is well-documented and fully replicable. The plan.md provides a clear scientific description of the methodology, while CodeWalkthrough.md gives practical implementation guidance. The source code is well-structured and includes sensible defaults. The experiment successfully replicated the core ELM methodology, demonstrating that the approach works as described.\n",
      "\n",
      "The main limitation for full replication is access to the WMDP-Bio dataset (which requires a special request), but the Harry Potter experiment serves as a viable alternative that is fully publicly accessible.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's also check the evaluation_replication.md for more context\n",
    "eval_replication_path = os.path.join(replication_path, 'evaluation_replication.md')\n",
    "with open(eval_replication_path, 'r') as f:\n",
    "    eval_replication_content = f.read()\n",
    "\n",
    "print(\"=== evaluation_replication.md ===\")\n",
    "print(eval_replication_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fa725ab",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## DE1. Result Fidelity Analysis\n",
      "\n",
      "### Original Documentation Results (from plan.md):\n",
      "1. **WMDP Experiments**:\n",
      "   - ELM achieves near-random performance on WMDP (Bio: 29.7-33.7%, Cyber: 26.6-28.2%)\n",
      "   - Maintains MMLU (75.2-78.8%) and MT-Bench (7.1-7.9) scores\n",
      "   - Better fluency (R-PPL 4.3-10.9) than baselines\n",
      "\n",
      "2. **Harry Potter Experiments** (plan.md):\n",
      "   - ELM achieves 38.3% HP-MCQ (better erasure than WHP 58.6% and RMU 51.0%)\n",
      "   - Maintains 45.3% MMLU and 3.4 R-PPL\n",
      "   \n",
      "3. **Ablation Study**:\n",
      "   - Lerase is crucial for erasure (w/o: 64.8% Bio vs. 29.7% with)\n",
      "   - Lretain vital for specificity (w/o: 23.6% MMLU vs. 56.6% with)\n",
      "   - Lfluency essential for coherence (w/o: 29.8 R-PPL vs. 11.0 with)\n",
      "\n",
      "4. **Configuration**:\n",
      "   - LoRA rank 4, η=500, layers 4-7 optimal\n",
      "\n",
      "### Replicated Documentation Results (from documentation_replication.md):\n",
      "1. **Harry Potter Experiments**:\n",
      "   - Training completed with 50 iterations (vs. 3000-5000 in paper)\n",
      "   - Loss curves converged during training\n",
      "   - Model maintains coherent generation ability\n",
      "   - Some Harry Potter knowledge persists due to limited training\n",
      "   \n",
      "2. **Qualitative Observations**:\n",
      "   - With limited training, erasure effect is subtle\n",
      "   - Results are deterministic with greedy decoding\n",
      "   \n",
      "3. **Configuration Used**:\n",
      "   - LoRA Rank: 256 (not 4 as optimal, but as recommended for literary domains)\n",
      "   - eta: 1000 (not 500 as optimal)\n",
      "   - Layers 4-7 (matches original)\n",
      "\n",
      "### Comparison:\n",
      "- The replication does NOT report quantitative metrics (MMLU, HP-MCQ, R-PPL) that the original documented\n",
      "- The replication acknowledges using limited training (50 vs. 3000-5000 iterations)\n",
      "- The replication uses different hyperparameters (rank 256, eta 1000 vs. rank 4, eta 500)\n",
      "- The replication honestly notes that \"erasure effect is subtle\" and \"Some Harry Potter knowledge persists\"\n",
      "\n",
      "### Verdict: The replication documentation does NOT report matching results because:\n",
      "1. It explicitly used reduced training scale\n",
      "2. It did not compute the same quantitative metrics\n",
      "3. It honestly acknowledges limitations\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now let's perform the detailed comparison for each evaluation criterion\n",
    "# Store our analysis\n",
    "\n",
    "# DE1. Result Fidelity Analysis\n",
    "# Comparing results between original (plan.md, CodeWalkthrough.md) and replication (documentation_replication.md)\n",
    "\n",
    "de1_analysis = \"\"\"\n",
    "## DE1. Result Fidelity Analysis\n",
    "\n",
    "### Original Documentation Results (from plan.md):\n",
    "1. **WMDP Experiments**:\n",
    "   - ELM achieves near-random performance on WMDP (Bio: 29.7-33.7%, Cyber: 26.6-28.2%)\n",
    "   - Maintains MMLU (75.2-78.8%) and MT-Bench (7.1-7.9) scores\n",
    "   - Better fluency (R-PPL 4.3-10.9) than baselines\n",
    "\n",
    "2. **Harry Potter Experiments** (plan.md):\n",
    "   - ELM achieves 38.3% HP-MCQ (better erasure than WHP 58.6% and RMU 51.0%)\n",
    "   - Maintains 45.3% MMLU and 3.4 R-PPL\n",
    "   \n",
    "3. **Ablation Study**:\n",
    "   - Lerase is crucial for erasure (w/o: 64.8% Bio vs. 29.7% with)\n",
    "   - Lretain vital for specificity (w/o: 23.6% MMLU vs. 56.6% with)\n",
    "   - Lfluency essential for coherence (w/o: 29.8 R-PPL vs. 11.0 with)\n",
    "\n",
    "4. **Configuration**:\n",
    "   - LoRA rank 4, η=500, layers 4-7 optimal\n",
    "\n",
    "### Replicated Documentation Results (from documentation_replication.md):\n",
    "1. **Harry Potter Experiments**:\n",
    "   - Training completed with 50 iterations (vs. 3000-5000 in paper)\n",
    "   - Loss curves converged during training\n",
    "   - Model maintains coherent generation ability\n",
    "   - Some Harry Potter knowledge persists due to limited training\n",
    "   \n",
    "2. **Qualitative Observations**:\n",
    "   - With limited training, erasure effect is subtle\n",
    "   - Results are deterministic with greedy decoding\n",
    "   \n",
    "3. **Configuration Used**:\n",
    "   - LoRA Rank: 256 (not 4 as optimal, but as recommended for literary domains)\n",
    "   - eta: 1000 (not 500 as optimal)\n",
    "   - Layers 4-7 (matches original)\n",
    "\n",
    "### Comparison:\n",
    "- The replication does NOT report quantitative metrics (MMLU, HP-MCQ, R-PPL) that the original documented\n",
    "- The replication acknowledges using limited training (50 vs. 3000-5000 iterations)\n",
    "- The replication uses different hyperparameters (rank 256, eta 1000 vs. rank 4, eta 500)\n",
    "- The replication honestly notes that \"erasure effect is subtle\" and \"Some Harry Potter knowledge persists\"\n",
    "\n",
    "### Verdict: The replication documentation does NOT report matching results because:\n",
    "1. It explicitly used reduced training scale\n",
    "2. It did not compute the same quantitative metrics\n",
    "3. It honestly acknowledges limitations\n",
    "\"\"\"\n",
    "\n",
    "print(de1_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0cbc6d8",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## DE2. Conclusion Consistency Analysis\n",
      "\n",
      "### Original Documentation Conclusions (from plan.md and CodeWalkthrough.md):\n",
      "\n",
      "1. **Core Claims**:\n",
      "   - ELM successfully erases conceptual knowledge while maintaining model capabilities\n",
      "   - Three loss components (Lerase, Lretain, Lfluency) are all essential for balanced erasure\n",
      "   - Early layers (4-7) are effective for factual knowledge modification\n",
      "   - The method resists adversarial attacks (GCG, BEAST)\n",
      "   \n",
      "2. **Method Effectiveness**:\n",
      "   - Achieves near-random performance on target concepts (WMDP-Bio/Cyber/HP)\n",
      "   - Maintains general capabilities (MMLU, MT-Bench)\n",
      "   - Produces fluent outputs (low R-PPL)\n",
      "\n",
      "3. **Key Insights**:\n",
      "   - Introspective classification using expert/novice prompts is effective\n",
      "   - Low-rank adapters on early layers enable precise knowledge modification\n",
      "\n",
      "### Replicated Documentation Conclusions (from documentation_replication.md):\n",
      "\n",
      "1. **What Worked**:\n",
      "   - \"ELM edit vector computation correctly implements the paper's formula\"\n",
      "   - \"Three-loss training framework functions as designed\"\n",
      "   - \"LoRA adapters successfully applied to early layers\"\n",
      "   - \"Model maintains fluency post-training\"\n",
      "\n",
      "2. **Key Insights**:\n",
      "   - \"Early layers (4-7) are effective targets for factual knowledge modification\"\n",
      "   - \"The three-loss balance is crucial for maintaining model utility\"\n",
      "   - \"Expert/novice prompting provides a natural way to define erasure targets\"\n",
      "\n",
      "3. **Limitations Acknowledged**:\n",
      "   - Limited training (50 vs. 3000-5000 iterations)\n",
      "   - No quantitative metrics (MMLU, HP-MCQ)\n",
      "   - Single concept tested (only Harry Potter, not WMDP-Bio/Cyber)\n",
      "\n",
      "### Comparison:\n",
      "The replicated conclusions are **CONSISTENT** with the original because:\n",
      "1. Both agree on the effectiveness of the three-loss framework\n",
      "2. Both confirm early layers (4-7) are effective for knowledge modification\n",
      "3. Both validate expert/novice prompting as an effective mechanism\n",
      "4. The replication honestly acknowledges its limitations rather than making false claims\n",
      "5. The replication does NOT contradict any original conclusions\n",
      "\n",
      "### Verdict: PASS - Conclusions are consistent, and limitations are transparently acknowledged\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DE2. Conclusion Consistency Analysis\n",
    "\n",
    "de2_analysis = \"\"\"\n",
    "## DE2. Conclusion Consistency Analysis\n",
    "\n",
    "### Original Documentation Conclusions (from plan.md and CodeWalkthrough.md):\n",
    "\n",
    "1. **Core Claims**:\n",
    "   - ELM successfully erases conceptual knowledge while maintaining model capabilities\n",
    "   - Three loss components (Lerase, Lretain, Lfluency) are all essential for balanced erasure\n",
    "   - Early layers (4-7) are effective for factual knowledge modification\n",
    "   - The method resists adversarial attacks (GCG, BEAST)\n",
    "   \n",
    "2. **Method Effectiveness**:\n",
    "   - Achieves near-random performance on target concepts (WMDP-Bio/Cyber/HP)\n",
    "   - Maintains general capabilities (MMLU, MT-Bench)\n",
    "   - Produces fluent outputs (low R-PPL)\n",
    "\n",
    "3. **Key Insights**:\n",
    "   - Introspective classification using expert/novice prompts is effective\n",
    "   - Low-rank adapters on early layers enable precise knowledge modification\n",
    "\n",
    "### Replicated Documentation Conclusions (from documentation_replication.md):\n",
    "\n",
    "1. **What Worked**:\n",
    "   - \"ELM edit vector computation correctly implements the paper's formula\"\n",
    "   - \"Three-loss training framework functions as designed\"\n",
    "   - \"LoRA adapters successfully applied to early layers\"\n",
    "   - \"Model maintains fluency post-training\"\n",
    "\n",
    "2. **Key Insights**:\n",
    "   - \"Early layers (4-7) are effective targets for factual knowledge modification\"\n",
    "   - \"The three-loss balance is crucial for maintaining model utility\"\n",
    "   - \"Expert/novice prompting provides a natural way to define erasure targets\"\n",
    "\n",
    "3. **Limitations Acknowledged**:\n",
    "   - Limited training (50 vs. 3000-5000 iterations)\n",
    "   - No quantitative metrics (MMLU, HP-MCQ)\n",
    "   - Single concept tested (only Harry Potter, not WMDP-Bio/Cyber)\n",
    "\n",
    "### Comparison:\n",
    "The replicated conclusions are **CONSISTENT** with the original because:\n",
    "1. Both agree on the effectiveness of the three-loss framework\n",
    "2. Both confirm early layers (4-7) are effective for knowledge modification\n",
    "3. Both validate expert/novice prompting as an effective mechanism\n",
    "4. The replication honestly acknowledges its limitations rather than making false claims\n",
    "5. The replication does NOT contradict any original conclusions\n",
    "\n",
    "### Verdict: PASS - Conclusions are consistent, and limitations are transparently acknowledged\n",
    "\"\"\"\n",
    "\n",
    "print(de2_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1da48613",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## DE3. No External or Hallucinated Information Analysis\n",
      "\n",
      "### Checking for External/Hallucinated Information in Replicated Documentation:\n",
      "\n",
      "1. **Method Description**:\n",
      "   - ELM formula: log P'(x) ∝ log P(x) + eta * (log P(x|c_novice) - log P(x|c_expert))\n",
      "   - ✓ This matches the original CodeWalkthrough.md exactly\n",
      "\n",
      "2. **Three Loss Components**:\n",
      "   - L_erase, L_retain, L_fluency described\n",
      "   - ✓ All are documented in the original plan.md\n",
      "\n",
      "3. **Model Configuration**:\n",
      "   - Base model: HuggingFaceH4/zephyr-7b-beta\n",
      "   - ✓ Mentioned in CodeWalkthrough.md\n",
      "   - LoRA rank 256 for literary domains\n",
      "   - ✓ Mentioned in original: \"lora_rank 256\" for Harry Potter\n",
      "   - Layers 4-7\n",
      "   - ✓ Documented in plan.md as optimal\n",
      "\n",
      "4. **Datasets**:\n",
      "   - Harry Potter: mickume/harry_potter_tiny\n",
      "   - ✓ Harry Potter domain is documented in original\n",
      "   - Wikipedia retain dataset: philschmid/easyrag-mini-wikipedia\n",
      "   - This is a reasonable substitute but not explicitly mentioned in original\n",
      "   \n",
      "5. **Hyperparameters**:\n",
      "   - eta: 1000 (original documents eta=500-1000)\n",
      "   - ✓ Within documented range\n",
      "   - Learning rate: 5e-5\n",
      "   - ✓ Standard value, not contradicting original\n",
      "\n",
      "6. **Claims Made**:\n",
      "   - All claims about \"what worked\" reference the original methodology\n",
      "   - Limitations are about the replication scope, not new findings\n",
      "   - No new research claims introduced\n",
      "\n",
      "### Potential Concerns:\n",
      "- The specific dataset \"philschmid/easyrag-mini-wikipedia\" is not in the original, but using Wikipedia for retain data is mentioned in original\n",
      "- \"100 samples\" scale is acknowledged as different from \"3000-5000\"\n",
      "- These are methodological adaptations, not hallucinated findings\n",
      "\n",
      "### Verdict: PASS\n",
      "- All core methodology accurately reflects original documentation\n",
      "- No external references that contradict or extend beyond original claims\n",
      "- No invented findings or hallucinated details\n",
      "- Dataset substitutions are transparent and reasonable for a replication\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DE3. No External or Hallucinated Information Analysis\n",
    "\n",
    "de3_analysis = \"\"\"\n",
    "## DE3. No External or Hallucinated Information Analysis\n",
    "\n",
    "### Checking for External/Hallucinated Information in Replicated Documentation:\n",
    "\n",
    "1. **Method Description**:\n",
    "   - ELM formula: log P'(x) ∝ log P(x) + eta * (log P(x|c_novice) - log P(x|c_expert))\n",
    "   - ✓ This matches the original CodeWalkthrough.md exactly\n",
    "\n",
    "2. **Three Loss Components**:\n",
    "   - L_erase, L_retain, L_fluency described\n",
    "   - ✓ All are documented in the original plan.md\n",
    "\n",
    "3. **Model Configuration**:\n",
    "   - Base model: HuggingFaceH4/zephyr-7b-beta\n",
    "   - ✓ Mentioned in CodeWalkthrough.md\n",
    "   - LoRA rank 256 for literary domains\n",
    "   - ✓ Mentioned in original: \"lora_rank 256\" for Harry Potter\n",
    "   - Layers 4-7\n",
    "   - ✓ Documented in plan.md as optimal\n",
    "\n",
    "4. **Datasets**:\n",
    "   - Harry Potter: mickume/harry_potter_tiny\n",
    "   - ✓ Harry Potter domain is documented in original\n",
    "   - Wikipedia retain dataset: philschmid/easyrag-mini-wikipedia\n",
    "   - This is a reasonable substitute but not explicitly mentioned in original\n",
    "   \n",
    "5. **Hyperparameters**:\n",
    "   - eta: 1000 (original documents eta=500-1000)\n",
    "   - ✓ Within documented range\n",
    "   - Learning rate: 5e-5\n",
    "   - ✓ Standard value, not contradicting original\n",
    "\n",
    "6. **Claims Made**:\n",
    "   - All claims about \"what worked\" reference the original methodology\n",
    "   - Limitations are about the replication scope, not new findings\n",
    "   - No new research claims introduced\n",
    "\n",
    "### Potential Concerns:\n",
    "- The specific dataset \"philschmid/easyrag-mini-wikipedia\" is not in the original, but using Wikipedia for retain data is mentioned in original\n",
    "- \"100 samples\" scale is acknowledged as different from \"3000-5000\"\n",
    "- These are methodological adaptations, not hallucinated findings\n",
    "\n",
    "### Verdict: PASS\n",
    "- All core methodology accurately reflects original documentation\n",
    "- No external references that contradict or extend beyond original claims\n",
    "- No invented findings or hallucinated details\n",
    "- Dataset substitutions are transparent and reasonable for a replication\n",
    "\"\"\"\n",
    "\n",
    "print(de3_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "347ec495",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DE1 (Result Fidelity): FAIL\n",
      "DE2 (Conclusion Consistency): PASS\n",
      "DE3 (No External Information): PASS\n",
      "\n",
      "Final Verdict: REVISION REQUIRED\n"
     ]
    }
   ],
   "source": [
    "# Determine final verdict for DE1\n",
    "# The key question: Does DE1 require the replication to achieve the same results,\n",
    "# or to faithfully report what was achieved?\n",
    "\n",
    "# Looking at the criterion more carefully:\n",
    "# \"PASS — Replicated documentation reports results (metrics, trends, qualitative findings) \n",
    "#  that match the original documentation within acceptable tolerance.\"\n",
    "\n",
    "# The replication:\n",
    "# 1. Did NOT run the same experiments at full scale (50 vs 3000-5000 iterations)\n",
    "# 2. Did NOT compute the same metrics (MMLU, HP-MCQ, R-PPL)\n",
    "# 3. Used different hyperparameters\n",
    "# 4. The results they did report (qualitative: \"subtle erasure\", \"knowledge persists\") \n",
    "#    do NOT match the original claims of strong erasure\n",
    "\n",
    "# This is a FAIL for DE1 because the reported results do not match the original.\n",
    "# However, the replication is honest about why - it's not a fabrication.\n",
    "\n",
    "de1_verdict = \"FAIL\"\n",
    "de1_rationale = \"\"\"The replicated documentation reports results that do not match the original within acceptable tolerance. \n",
    "The original documents strong erasure (38.3% HP-MCQ, near-random on WMDP) with maintained general capabilities, \n",
    "while the replication reports only \"subtle\" erasure effects with \"Some Harry Potter knowledge persists.\" \n",
    "The replication did not compute the same quantitative metrics (MMLU, HP-MCQ, R-PPL) and used reduced training scale (50 vs 3000-5000 iterations), \n",
    "making direct comparison impossible. While the replication is honest about its limitations, the results do not match.\"\"\"\n",
    "\n",
    "de2_verdict = \"PASS\"\n",
    "de2_rationale = \"\"\"The replicated documentation presents conclusions consistent with the original. \n",
    "Both agree on: (1) the three-loss framework is effective and crucial, (2) early layers (4-7) are effective targets, \n",
    "(3) expert/novice prompting works as designed. The replication honestly acknowledges its limitations \n",
    "(reduced training, no quantitative metrics) rather than making false claims. \n",
    "No conclusions contradict the original, and essential claims are preserved.\"\"\"\n",
    "\n",
    "de3_verdict = \"PASS\"\n",
    "de3_rationale = \"\"\"No external or hallucinated information appears in the replicated documentation. \n",
    "The ELM formula, three-loss framework, LoRA configuration, and methodology all accurately reflect the original. \n",
    "Dataset substitutions (using publicly available Harry Potter and Wikipedia datasets) are transparent and reasonable. \n",
    "All claims reference the original methodology without introducing invented findings or external references.\"\"\"\n",
    "\n",
    "print(f\"DE1 (Result Fidelity): {de1_verdict}\")\n",
    "print(f\"DE2 (Conclusion Consistency): {de2_verdict}\")\n",
    "print(f\"DE3 (No External Information): {de3_verdict}\")\n",
    "\n",
    "# Final verdict\n",
    "if de1_verdict == \"PASS\" and de2_verdict == \"PASS\" and de3_verdict == \"PASS\":\n",
    "    final_verdict = \"PASS\"\n",
    "else:\n",
    "    final_verdict = \"REVISION REQUIRED\"\n",
    "    \n",
    "print(f\"\\nFinal Verdict: {final_verdict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c91d184",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written: /net/scratch2/smallyan/erasing-llm_eval/evaluation/replication_eval/documentation_evaluation_summary.md\n",
      "File size: 4221 bytes\n"
     ]
    }
   ],
   "source": [
    "# Create documentation_evaluation_summary.md\n",
    "from datetime import datetime\n",
    "\n",
    "summary_md = f\"\"\"# Documentation Evaluation Summary\n",
    "\n",
    "**Evaluation Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "**Original Documentation**: \n",
    "- `/net/scratch2/smallyan/erasing-llm_eval/CodeWalkthrough.md`\n",
    "- `/net/scratch2/smallyan/erasing-llm_eval/plan.md`\n",
    "\n",
    "**Replicated Documentation**: \n",
    "- `/net/scratch2/smallyan/erasing-llm_eval/evaluation/replications/documentation_replication.md`\n",
    "\n",
    "---\n",
    "\n",
    "## Results Comparison\n",
    "\n",
    "The original documentation (plan.md) reports strong quantitative results for the ELM method:\n",
    "- **WMDP Experiments**: Near-random performance (Bio: 29.7-33.7%, Cyber: 26.6-28.2%) with maintained MMLU (75.2-78.8%) and MT-Bench (7.1-7.9) scores\n",
    "- **Harry Potter Experiments**: 38.3% HP-MCQ accuracy with 45.3% MMLU and 3.4 R-PPL\n",
    "- **Ablation Study**: Clear evidence that all three loss components (L_erase, L_retain, L_fluency) are essential\n",
    "\n",
    "The replicated documentation reports qualitative results with acknowledged limitations:\n",
    "- Training completed with only 50 iterations (vs. 3000-5000 in original)\n",
    "- Erasure effect described as \"subtle\" with \"Some Harry Potter knowledge persists\"\n",
    "- No quantitative metrics (MMLU, HP-MCQ, R-PPL) were computed\n",
    "- Different hyperparameters used (LoRA rank 256 vs. optimal rank 4, eta 1000 vs. 500)\n",
    "\n",
    "**Assessment**: The results do not match because the replication was run at reduced scale without computing equivalent metrics. While the replication honestly acknowledges these limitations, the documented results cannot be confirmed to match the original within acceptable tolerance.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusions Comparison\n",
    "\n",
    "Both the original and replicated documentation share consistent conclusions about the ELM methodology:\n",
    "\n",
    "1. **Three-Loss Framework**: Both confirm the effectiveness and necessity of L_erase, L_retain, and L_fluency components\n",
    "2. **Early Layer Targeting**: Both validate that layers 4-7 are effective for factual knowledge modification\n",
    "3. **Expert/Novice Prompting**: Both support this approach as an effective mechanism for defining erasure targets\n",
    "4. **Model Fluency**: Both observe that the model maintains coherent generation ability\n",
    "\n",
    "The replication does not contradict any original conclusions and transparently acknowledges its scope limitations.\n",
    "\n",
    "**Assessment**: Conclusions are consistent. The replication validates the methodology's core principles without making claims that extend beyond or contradict the original.\n",
    "\n",
    "---\n",
    "\n",
    "## External/Hallucinated Information Check\n",
    "\n",
    "The replicated documentation accurately reflects the original methodology:\n",
    "- The ELM formula matches the original exactly\n",
    "- Three loss components are correctly described\n",
    "- LoRA configuration aligns with documented recommendations\n",
    "- Hyperparameters fall within documented ranges\n",
    "\n",
    "Minor adaptations were made transparently:\n",
    "- Used publicly available Harry Potter dataset instead of WMDP-Bio (which requires special access)\n",
    "- Used Wikipedia dataset for retention (consistent with original approach)\n",
    "- Reduced sample size (100 vs. 3000-5000) clearly documented\n",
    "\n",
    "**Assessment**: No external references, invented findings, or hallucinated details were introduced. All methodological adaptations are transparent and reasonable.\n",
    "\n",
    "---\n",
    "\n",
    "## Evaluation Checklist Summary\n",
    "\n",
    "| Criterion | Result | Notes |\n",
    "|-----------|--------|-------|\n",
    "| **DE1. Result Fidelity** | **FAIL** | Results do not match due to reduced training scale and missing quantitative metrics |\n",
    "| **DE2. Conclusion Consistency** | **PASS** | Conclusions are consistent with original; limitations transparently acknowledged |\n",
    "| **DE3. No External Information** | **PASS** | No hallucinated or external information introduced |\n",
    "\n",
    "---\n",
    "\n",
    "## Final Verdict: **REVISION REQUIRED**\n",
    "\n",
    "The replicated documentation fails DE1 (Result Fidelity) because the reported results do not match the original documentation. To achieve PASS status, the replication should:\n",
    "\n",
    "1. **Run full-scale training** (3000-5000 iterations as in the original)\n",
    "2. **Compute quantitative metrics** (HP-MCQ accuracy, MMLU, R-PPL)\n",
    "3. **Use optimal hyperparameters** as documented (LoRA rank 4, eta 500)\n",
    "4. **Report results in comparable format** to enable direct comparison\n",
    "\n",
    "The replication successfully demonstrates the methodology works but does not reproduce the claimed results at comparable scale.\n",
    "\"\"\"\n",
    "\n",
    "# Write to file\n",
    "output_file = os.path.join(output_path, 'documentation_evaluation_summary.md')\n",
    "with open(output_file, 'w') as f:\n",
    "    f.write(summary_md)\n",
    "\n",
    "print(f\"Written: {output_file}\")\n",
    "print(f\"File size: {os.path.getsize(output_file)} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25c8482d",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written: /net/scratch2/smallyan/erasing-llm_eval/evaluation/replication_eval/documentation_eval_summary.json\n",
      "File size: 1797 bytes\n",
      "\n",
      "JSON content:\n",
      "{\n",
      "  \"Checklist\": {\n",
      "    \"DE1_ResultFidelity\": \"FAIL\",\n",
      "    \"DE2_ConclusionConsistency\": \"PASS\",\n",
      "    \"DE3_NoExternalInformation\": \"PASS\"\n",
      "  },\n",
      "  \"Rationale\": {\n",
      "    \"DE1_ResultFidelity\": \"The replicated documentation reports results that do not match the original within acceptable tolerance. The original documents strong erasure (38.3% HP-MCQ, near-random on WMDP) with maintained general capabilities, while the replication reports only 'subtle' erasure effects with 'Some Harry Potter knowledge persists.' The replication did not compute the same quantitative metrics (MMLU, HP-MCQ, R-PPL) and used reduced training scale (50 vs 3000-5000 iterations), making direct comparison impossible. While the replication is honest about its limitations, the results do not match.\",\n",
      "    \"DE2_ConclusionConsistency\": \"The replicated documentation presents conclusions consistent with the original. Both agree on: (1) the three-loss framework is effective and crucial, (2) early layers (4-7) are effective targets, (3) expert/novice prompting works as designed. The replication honestly acknowledges its limitations (reduced training, no quantitative metrics) rather than making false claims. No conclusions contradict the original, and essential claims are preserved.\",\n",
      "    \"DE3_NoExternalInformation\": \"No external or hallucinated information appears in the replicated documentation. The ELM formula, three-loss framework, LoRA configuration, and methodology all accurately reflect the original. Dataset substitutions (using publicly available Harry Potter and Wikipedia datasets) are transparent and reasonable. All claims reference the original methodology without introducing invented findings or external references.\"\n",
      "  },\n",
      "  \"FinalVerdict\": \"REVISION REQUIRED\",\n",
      "  \"EvaluationDate\": \"2025-12-24 00:58:55\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Create documentation_eval_summary.json\n",
    "import json\n",
    "\n",
    "eval_json = {\n",
    "    \"Checklist\": {\n",
    "        \"DE1_ResultFidelity\": de1_verdict,\n",
    "        \"DE2_ConclusionConsistency\": de2_verdict,\n",
    "        \"DE3_NoExternalInformation\": de3_verdict\n",
    "    },\n",
    "    \"Rationale\": {\n",
    "        \"DE1_ResultFidelity\": \"The replicated documentation reports results that do not match the original within acceptable tolerance. The original documents strong erasure (38.3% HP-MCQ, near-random on WMDP) with maintained general capabilities, while the replication reports only 'subtle' erasure effects with 'Some Harry Potter knowledge persists.' The replication did not compute the same quantitative metrics (MMLU, HP-MCQ, R-PPL) and used reduced training scale (50 vs 3000-5000 iterations), making direct comparison impossible. While the replication is honest about its limitations, the results do not match.\",\n",
    "        \"DE2_ConclusionConsistency\": \"The replicated documentation presents conclusions consistent with the original. Both agree on: (1) the three-loss framework is effective and crucial, (2) early layers (4-7) are effective targets, (3) expert/novice prompting works as designed. The replication honestly acknowledges its limitations (reduced training, no quantitative metrics) rather than making false claims. No conclusions contradict the original, and essential claims are preserved.\",\n",
    "        \"DE3_NoExternalInformation\": \"No external or hallucinated information appears in the replicated documentation. The ELM formula, three-loss framework, LoRA configuration, and methodology all accurately reflect the original. Dataset substitutions (using publicly available Harry Potter and Wikipedia datasets) are transparent and reasonable. All claims reference the original methodology without introducing invented findings or external references.\"\n",
    "    },\n",
    "    \"FinalVerdict\": final_verdict,\n",
    "    \"EvaluationDate\": datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "}\n",
    "\n",
    "# Write to file\n",
    "json_file = os.path.join(output_path, 'documentation_eval_summary.json')\n",
    "with open(json_file, 'w') as f:\n",
    "    json.dump(eval_json, f, indent=2)\n",
    "\n",
    "print(f\"Written: {json_file}\")\n",
    "print(f\"File size: {os.path.getsize(json_file)} bytes\")\n",
    "print(\"\\nJSON content:\")\n",
    "print(json.dumps(eval_json, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7368a06a",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output files created:\n",
      "  documentation_eval_summary.json (1797 bytes)\n",
      "  documentation_evaluation_summary.md (4221 bytes)\n",
      "\n",
      "============================================================\n",
      "DOCUMENTATION EVALUATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Output directory: /net/scratch2/smallyan/erasing-llm_eval/evaluation/replication_eval\n",
      "\n",
      "Files generated:\n",
      "  1. documentation_evaluation_summary.md\n",
      "  2. documentation_eval_summary.json\n",
      "\n",
      "Final Verdict: REVISION REQUIRED\n",
      "\n",
      "DE1 (Result Fidelity): FAIL - Results do not match due to reduced scale\n",
      "DE2 (Conclusion Consistency): PASS - Conclusions are consistent\n",
      "DE3 (No External Information): PASS - No hallucinated content\n"
     ]
    }
   ],
   "source": [
    "# Verify all output files were created\n",
    "print(\"Output files created:\")\n",
    "for f in os.listdir(output_path):\n",
    "    full_path = os.path.join(output_path, f)\n",
    "    print(f\"  {f} ({os.path.getsize(full_path)} bytes)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DOCUMENTATION EVALUATION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nOutput directory: {output_path}\")\n",
    "print(f\"\\nFiles generated:\")\n",
    "print(\"  1. documentation_evaluation_summary.md\")\n",
    "print(\"  2. documentation_eval_summary.json\")\n",
    "print(f\"\\nFinal Verdict: {final_verdict}\")\n",
    "print(\"\\nDE1 (Result Fidelity): FAIL - Results do not match due to reduced scale\")\n",
    "print(\"DE2 (Conclusion Consistency): PASS - Conclusions are consistent\")\n",
    "print(\"DE3 (No External Information): PASS - No hallucinated content\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73c1849",
   "metadata": {},
   "source": [
    "# Documentation Evaluation Summary\n",
    "\n",
    "## Task\n",
    "Evaluate whether the replicator's documentation faithfully reproduces the results and conclusions of the original ELM (Erasure of Language Memory) experiment.\n",
    "\n",
    "## Results\n",
    "\n",
    "| Criterion | Result |\n",
    "|-----------|--------|\n",
    "| DE1. Result Fidelity | **FAIL** |\n",
    "| DE2. Conclusion Consistency | **PASS** |\n",
    "| DE3. No External Information | **PASS** |\n",
    "\n",
    "## Final Verdict: **REVISION REQUIRED**\n",
    "\n",
    "### Key Findings:\n",
    "- **DE1 Failed**: The replication used reduced training scale (50 vs 3000-5000 iterations) and did not compute quantitative metrics (MMLU, HP-MCQ, R-PPL). The reported \"subtle\" erasure does not match the original's strong erasure claims.\n",
    "- **DE2 Passed**: Conclusions about the three-loss framework, early layer targeting, and expert/novice prompting are consistent with the original.\n",
    "- **DE3 Passed**: No hallucinated or external information was introduced; all methodology accurately reflects the original.\n",
    "\n",
    "### Output Files:\n",
    "- `documentation_evaluation_summary.md`\n",
    "- `documentation_eval_summary.json`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scribe: 2025-12-24-00-54_circuit_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
